{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymer Chemistry Informed Neural Networks (PCINNs)\n",
    "\n",
    "**Paper:** Ballard, N. *Polymer Chemistry Informed Neural Networks.* *Polym. Chem.*, 2024. DOI: [10.1039/D4PY00995A](https://doi.org/10.1039/D4PY00995A)\n",
    "\n",
    "**GitHub:** [PolymatGIQ/PCINN](https://github.com/PolymatGIQ/PCINN)\n",
    "\n",
    "This notebook implements the PCINN method for predicting methyl methacrylate (MMA) free-radical polymerization outcomes. The approach combines a data-driven neural network with Jacobian-based regularization from a pretrained kinetic (\"theory\") model. The theory model provides gradient information that guides the trainable network toward physically consistent predictions, yielding improved generalization compared to a purely data-driven baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.10.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy==2.4.2 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib==3.10.8 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: openpyxl==3.1.5 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: ipykernel<7,>=6.29 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (6.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (3.24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (2026.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from torch==2.10.0) (82.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from pandas==2.2.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from pandas==2.2.3) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from matplotlib==3.10.8) (3.3.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from openpyxl==3.1.5) (2.0.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (1.8.20)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (9.10.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (8.8.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (7.2.2)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (6.5.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipykernel<7,>=6.29) (5.14.3)\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel<7,>=6.29) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel<7,>=6.29) (0.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel<7,>=6.29) (0.8.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel<7,>=6.29) (4.9.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel<7,>=6.29) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel<7,>=6.29) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel<7,>=6.29) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.10.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ari\\desktop\\github projects\\pcinn\\.venv\\lib\\site-packages (from jinja2->torch==2.10.0) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install pinned dependencies (verified compatible as of February 2026).\n",
    "# On systems with CUDA, follow https://pytorch.org/get-started/locally/\n",
    "# to install the CUDA-enabled PyTorch build instead of the default CPU-only wheel.\n",
    "# If a pin fails to resolve, check PyPI for the latest patch in the same minor\n",
    "# series (e.g., numpy==2.4.3 if 2.4.2 is superseded).\n",
    "%pip install torch==2.10.0 numpy==2.4.2 pandas==2.2.3 matplotlib==3.10.8 openpyxl==3.1.5 \"ipykernel>=6.29,<7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "# UNSAFE WORKAROUND — only enable if you hit OpenMP duplication errors.\n",
    "# Can crash or silently produce incorrect results. Must be set before\n",
    "# NumPy/PyTorch (and other heavy numeric libs) are imported in a fresh kernel.\n",
    "ENABLE_UNSAFE_KMP_WORKAROUND = False\n",
    "\n",
    "if ENABLE_UNSAFE_KMP_WORKAROUND:\n",
    "    os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.func import jacrev, vmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Reproducibility & Device\n\nSeed all RNGs for reproducible results across runs. GPU is used when available.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "SEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Optional strictness (may slow things down / error on nondeterministic ops):\n# torch.use_deterministic_algorithms(True)\n# torch.backends.cudnn.benchmark = False\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "Two neural network architectures are used. **`NNmodel`** is the trainable predictive network (5 inputs → 128 → 64 → 6 outputs, tanh activations) used for both the baseline and PCINN experiments. **`DomainModel`** is the pretrained theory surrogate (5 → 128 → 128 → 64 with split output heads: sigmoid for conversion, softplus for molecular weights) that provides Jacobian targets during PCINN training. See paper Scheme 2 (theory model) and Scheme 3 (PCINN architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NNmodel(nn.Module):\n    \"\"\"Data-driven predictive network for MMA polymerization.\n\n    Architecture: 5 → 128 → 64 → 6 (tanh activations).\n\n    Inputs:  (batch, 5) — [M], [S], [I], temp, time  (all scaled 0–1)\n    Outputs: (batch, 6) — X (conversion), log10(Mn), log10(Mw),\n             log10(Mz), log10(Mz+1), log10(Mv)\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.fc1 = nn.Linear(5, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 6)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass DomainModel(nn.Module):\n    \"\"\"Pretrained kinetic theory surrogate (\"domain model\").\n\n    Architecture: 5 → 128 → 128 → 64 with split output heads.\n\n    Inputs:  (batch, 5) — same as NNmodel\n    Outputs: (batch, 6) — X via sigmoid ∈ [0, 1],\n             Mn/Mw/Mz/Mz+1/Mv via softplus > 0  (physical units)\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.fc1 = nn.Linear(5, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.Xout = nn.Linear(64, 1)\n        self.Mout = nn.Linear(64, 5)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n\n        X_out = torch.sigmoid(self.Xout(x))      # conversion\n        M_out = torch.nn.functional.softplus(self.Mout(x))  # Mn, Mw, Mz, Mz+1, Mv\n\n        return torch.cat((X_out, M_out), dim=-1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Input features are scaled to the [0, 1] range using precomputed per-feature min and max values stored in `scalerx_min.npy` and `scalerx_max.npy`. The five molecular weight outputs (Mn, Mw, Mz, Mz+1, Mv) are log10-transformed before training to reduce scale differences, while conversion (X) is left on its original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def scalefeaturezeroone(\n    x: np.ndarray,\n    scalerxmax: np.ndarray,\n    scalerxmin: np.ndarray,\n    *,\n    eps: float = 1e-12,\n) -> np.ndarray:\n    \"\"\"Min-max scale features to [0, 1] with numerical safety.\"\"\"\n    denom = np.maximum(scalerxmax - scalerxmin, eps)\n    return (x - scalerxmin) / denom"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "scalerx_max_np = np.load('scalerx_max.npy')\nscalerx_min_np = np.load('scalerx_min.npy')\n\n# Torch versions for use inside training loops (avoids NumPy/Torch interop warnings)\nSCALERX_MAX = torch.from_numpy(scalerx_max_np).float().to(DEVICE)\nSCALERX_MIN = torch.from_numpy(scalerx_min_np).float().to(DEVICE)\nSCALERX_DENOM = torch.clamp(SCALERX_MAX - SCALERX_MIN, min=1e-12)\n\n# Load training data\ndf = pd.read_excel('PMMAordered.xlsx')\n\ndfX = df[[\"[M]\", \"[S]\", \"[I]\", \"temp\", \"time\", \"Reaction\"]]\ndfY = df[[\"X\", \"Mn\", \"Mw\", \"Mz\", \"Mzplus1\", \"Mv\"]]\n\nXdata = dfX.values\nYdata = dfY.values\n\nYdata[:, 1:] = np.log10(Ydata[:, 1:])\nXdata[:, :5] = scalefeaturezeroone(Xdata[:, :5], scalerx_max_np, scalerx_min_np)\n\ndfGPC = df.iloc[:, 18:]\nGPCdata = dfGPC.values\n\n# Load and freeze pretrained theory model\nDomain_NN = DomainModel().to(DEVICE)\n\ntry:\n    Domain_NN.load_state_dict(\n        torch.load('MMA_solution_net.pt', weights_only=True, map_location=DEVICE)\n    )\nexcept pickle.UnpicklingError:\n    Domain_NN.load_state_dict(\n        torch.load('MMA_solution_net.pt', weights_only=False, map_location=DEVICE)\n    )\n\nDomain_NN.eval()\nfor p in Domain_NN.parameters():\n    p.requires_grad_(False)\n\nprint(f\"Domain_NN on {next(Domain_NN.parameters()).device}, eval={not Domain_NN.training}\")\nprint(f\"SCALERX_DENOM min={SCALERX_DENOM.min().item():.2e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Sampling Configuration\n",
    "\n",
    "During PCINN training, 32 random points are sampled per epoch from the physical domain to compute Jacobian matching loss. The sampling ranges correspond to experimentally relevant conditions: temperature 323–363 K (50–90 °C), monomer concentration [M] 0.5–5 mol/L, initiator concentration [I] 0.005–0.1 mol/L, and reaction time 300–36,000 s (5 min to 10 hours). See paper eqn (9) for the combined loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tupper = 273 + 90\n",
    "Tlower = 273 + 50\n",
    "\n",
    "Mupper = 5\n",
    "Mlower = 0.5\n",
    "\n",
    "Iupper = 0.1\n",
    "Ilower = 0.005\n",
    "\n",
    "timeupper = 10 * 60 * 60\n",
    "timelower = 5 * 60\n",
    "\n",
    "M_sampler = torch.distributions.Uniform(low=Mlower, high=Mupper)\n",
    "T_sampler = torch.distributions.Uniform(low=Tlower, high=Tupper)\n",
    "I_sampler = torch.distributions.Uniform(low=Ilower, high=Iupper)\n",
    "time_sampler = torch.distributions.Uniform(low=timelower, high=timeupper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "A leave-one-experiment-out cross-validation strategy is used: the dataset contains 8 distinct polymerization reactions, and `TestReaction` selects which reaction is held out for testing while the remaining 7 are used for training. To reproduce the full cross-validation results from paper Table 3, run this notebook 8 times with `TestReaction` set to each value from 1 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TestReaction = 8  # Integer between 1 and 8. Picks reaction that will be tested on (other reactions for training)\n\n# Convert train/test splits to tensors on DEVICE once (no repeated torch.from_numpy in loops)\nXtrainsample = torch.from_numpy(Xdata[Xdata[:, 5] != TestReaction, :5]).float().to(DEVICE)\nYtrainsample = torch.from_numpy(Ydata[Xdata[:, 5] != TestReaction]).float().to(DEVICE)\n\nXtestsample = torch.from_numpy(Xdata[Xdata[:, 5] == TestReaction, :5]).float().to(DEVICE)\nYtestsample = torch.from_numpy(Ydata[Xdata[:, 5] == TestReaction]).float().to(DEVICE)\n\nprint(f\"Train: {Xtrainsample.shape}, Test: {Xtestsample.shape}, device={Xtrainsample.device}\")"
  },
  {
   "cell_type": "code",
   "source": "def safe_log(x: list[float], eps: float = 1e-12) -> np.ndarray:\n    \"\"\"Numerically safe natural log for loss curves (clamps near-zero values).\"\"\"\n    arr = np.asarray(x, dtype=np.float64)\n    return np.log(np.clip(arr, eps, None))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Neural Network Training\n",
    "\n",
    "This cell trains a conventional data-only neural network (no Jacobian regularization) as a control comparison. The model is trained for 10,000 epochs with Adam (lr=3e-4) and MSE loss on the training data only. This serves as the baseline against which the PCINN is compared in paper Table 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# [ORIGINAL — paper baseline, do not execute]\n# Kept for reproducibility comparison. Use the improved cell below instead.\n\n# model = NNmodel()\n# loss_function = nn.MSELoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=3E-4)\n#\n# epochs = 10000\n# reg_losses = []\n# reg_losses_test = []\n# for epoch in range(epochs):\n#     Sum_Obj_loss = 0\n#     pred = model(Xtrainsample)\n#     trainloss = loss_function(pred, Ytrainsample)\n#     Sum_Obj_loss += trainloss\n#     if epoch >= 1:\n#         trainloss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n#\n#     pred = model(Xtestsample)\n#     testloss = loss_function(pred, torch.from_numpy(Ytestsample).float())\n#     reg_losses.append(float(Sum_Obj_loss))\n#     reg_losses_test.append(float(testloss))\n#\n# NNpred = model(Xtestsample)\n#\n# plt.plot(np.log(reg_losses), label='Training loss')\n# plt.plot(np.log(reg_losses_test), label='Test loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()"
  },
  {
   "cell_type": "code",
   "source": "model = NNmodel().to(DEVICE)\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\nEPOCHS = 10_000\ntrain_losses: list[float] = []\ntest_losses: list[float] = []\n\nfor epoch in range(EPOCHS):\n    model.train()\n    optimizer.zero_grad(set_to_none=True)\n\n    pred = model(Xtrainsample)\n    trainloss = loss_function(pred, Ytrainsample)\n    trainloss.backward()\n    optimizer.step()\n\n    train_losses.append(trainloss.item())\n\n    model.eval()\n    with torch.inference_mode():\n        test_pred = model(Xtestsample)\n        testloss = loss_function(test_pred, Ytestsample).item()\n    test_losses.append(testloss)\n\n    if (epoch + 1) % 500 == 0:\n        print(f\"[Baseline] epoch={epoch+1}/{EPOCHS} train={train_losses[-1]:.6g} test={test_losses[-1]:.6g}\")\n\nNNpred = model(Xtestsample)\n\nplt.figure()\nplt.plot(safe_log(train_losses), label=\"train (log)\")\nplt.plot(safe_log(test_losses), label=\"test (log)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"log(loss)\")\nplt.title(\"Baseline NN training curves (log scale)\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCINN Training\n",
    "\n",
    "The PCINN augments the standard data-driven MSE loss with a Jacobian matching term (see paper eqn 9). At each epoch, 32 random points are sampled from the physical domain and the Jacobian of both the trainable network and the pretrained theory network are computed using `vmap(jacrev(...))`. The MSE between these Jacobians is added to the data loss, encouraging the PCINN to learn input-output sensitivities consistent with the underlying kinetic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# [ORIGINAL — paper baseline, do not execute]\n# Kept for reproducibility comparison. Use the improved cell below instead.\n\n# PCINNmodel = NNmodel()\n# loss_function = nn.MSELoss()\n# optimizer = torch.optim.Adam(PCINNmodel.parameters(), lr=3E-4)\n#\n# epochs = 10000\n# reg_losses = []\n# reg_losses_test = []\n# totaljacsamples = 32\n#\n# for epoch in range(epochs):\n#     Sum_Jac_loss = 0\n#     Sum_Obj_loss = 0\n#\n#     pred = PCINNmodel(Xtrainsample)\n#     trainloss = loss_function(pred, Ytrainsample)\n#     Sum_Obj_loss += trainloss\n#\n#     Msample = M_sampler.sample((32, 1))\n#     Ssample = 10 - Msample\n#     Isample = I_sampler.sample((32, 1))\n#     Tsample = T_sampler.sample((32, 1))\n#     tsample = time_sampler.sample((32, 1))\n#     sampl = torch.cat((Msample, Ssample, Isample, Tsample, tsample), 1)\n#     sampl = (sampl - scalerx_min) / (scalerx_max - scalerx_min)\n#\n#     jac_theory_sampl = vmap(jacrev(Domain_NN))(sampl.float())\n#     jac_sampl = vmap(jacrev(PCINNmodel))(sampl.float())\n#\n#     jacloss = loss_function(jac_sampl, jac_theory_sampl)\n#     Sum_Jac_loss += jacloss\n#\n#     loss = trainloss + jacloss\n#\n#     if epoch >= 1:\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n#\n#     reg_losses.append(float(Sum_Obj_loss))\n#     pred = PCINNmodel(Xtestsample)\n#     testloss = loss_function(pred, torch.from_numpy(Ytestsample).float())\n#     reg_losses_test.append(float(testloss))\n#\n# EBNNpred = PCINNmodel(Xtestsample)\n#\n# plt.plot(np.log(reg_losses), label='Training loss')\n# plt.plot(np.log(reg_losses_test), label='Test loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()"
  },
  {
   "cell_type": "code",
   "source": "PCINNmodel = NNmodel().to(DEVICE)\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(PCINNmodel.parameters(), lr=3e-4)\n\nEPOCHS = 10_000\nTOTAL_JAC_SAMPLES = 32\n\npcinn_train_losses: list[float] = []\npcinn_test_losses: list[float] = []\njac_losses: list[float] = []\n\nfor epoch in range(EPOCHS):\n    PCINNmodel.train()\n    optimizer.zero_grad(set_to_none=True)\n\n    pred = PCINNmodel(Xtrainsample)\n    trainloss = loss_function(pred, Ytrainsample)\n\n    # --- Jacobian sampling ---\n    Msample = M_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    Ssample = 10 - Msample\n    Isample = I_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    Tsample = T_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    tsample = time_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n\n    sampl = torch.cat((Msample, Ssample, Isample, Tsample, tsample), dim=1).to(DEVICE)\n    sampl = (sampl - SCALERX_MIN) / SCALERX_DENOM\n\n    # Ensure sampl is a leaf tensor that can be differentiated\n    sampl = sampl.float().detach().requires_grad_(True)\n\n    # Theory Jacobian: grad enabled (needed for jacrev) but params frozen; detach as target\n    jac_theory_sampl = vmap(jacrev(Domain_NN))(sampl).detach()\n    # PCINN Jacobian: requires grad so jacloss backprops into PCINN params\n    jac_sampl = vmap(jacrev(PCINNmodel))(sampl)\n\n    jacloss = loss_function(jac_sampl, jac_theory_sampl)\n\n    loss = trainloss + jacloss\n    loss.backward()\n    optimizer.step()\n\n    pcinn_train_losses.append(trainloss.item())\n    jac_losses.append(jacloss.item())\n\n    PCINNmodel.eval()\n    with torch.inference_mode():\n        test_pred = PCINNmodel(Xtestsample)\n        testloss = loss_function(test_pred, Ytestsample).item()\n    pcinn_test_losses.append(testloss)\n\n    if (epoch + 1) % 500 == 0:\n        print(\n            f\"[PCINN] epoch={epoch+1}/{EPOCHS} \"\n            f\"obj={pcinn_train_losses[-1]:.6g} jac={jac_losses[-1]:.6g} test={pcinn_test_losses[-1]:.6g}\"\n        )\n\nEBNNpred = PCINNmodel(Xtestsample)\n\nplt.figure()\nplt.plot(safe_log(pcinn_train_losses), label=\"train (log)\")\nplt.plot(safe_log(pcinn_test_losses), label=\"test (log)\")\nplt.plot(safe_log(jac_losses), label=\"jac (log)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"log(loss)\")\nplt.title(\"PCINN training curves (log scale)\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model Export\n\nSave the trained PCINN weights, an inference bundle (weights + scaling parameters + metadata), and a human-readable JSON metadata file. A smoke-test reload verifies the saved weights produce identical predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "EXPORT_DIR = Path(\"exports\")\nEXPORT_DIR.mkdir(parents=True, exist_ok=True)\n\nfold = int(TestReaction)\n\nweights_path = EXPORT_DIR / f\"pcinn_fold{fold}_weights.pt\"\nbundle_path = EXPORT_DIR / f\"pcinn_fold{fold}_bundle.pt\"\nmeta_path = EXPORT_DIR / f\"pcinn_fold{fold}_meta.json\"\n\nPCINNmodel.eval()\n\n# 1) Weights only\ntorch.save(PCINNmodel.state_dict(), weights_path)\n\n# 2) Full inference bundle\nbundle = {\n    \"model_class\": \"NNmodel\",\n    \"model_state_dict\": copy.deepcopy(PCINNmodel.state_dict()),\n    \"scalerx_min\": scalerx_min_np,\n    \"scalerx_max\": scalerx_max_np,\n    \"y_log10_applied_to_columns_1_to_end\": True,\n    \"fold\": fold,\n    \"pytorch_version\": torch.__version__,\n}\ntorch.save(bundle, bundle_path)\n\n# 3) Human-readable metadata\nmeta = {\n    \"fold\": fold,\n    \"epochs\": int(EPOCHS),\n    \"lr\": 3e-4,\n    \"total_jac_samples\": int(TOTAL_JAC_SAMPLES),\n    \"pytorch_version\": torch.__version__,\n}\nmeta_path.write_text(json.dumps(meta, indent=2))\n\nprint(f\"Saved:\\n- {weights_path}\\n- {bundle_path}\\n- {meta_path}\")\n\n# 4) Smoke-test reload\nreloaded = NNmodel().to(DEVICE)\nreloaded.load_state_dict(torch.load(weights_path, weights_only=True, map_location=DEVICE))\nreloaded.eval()\n\nwith torch.inference_mode():\n    a = PCINNmodel(Xtestsample)\n    b = reloaded(Xtestsample)\nmax_diff = (a - b).abs().max().item()\nassert max_diff < 1e-6, f\"Reload mismatch: max diff={max_diff}\"\nprint(f\"Reload OK (max abs diff = {max_diff:.2e})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## PCINNv2 — Upgraded Architecture\n\nAn experimental variant that combines several architecture upgrades from the improvement plan:\n\n1. **Residual correction** (2.3.2): `y(x) = theory(x) + delta(concat(x, theory(x)))` — the NN learns only what the theory model misses.\n2. **Physical output heads** (2.3.1): `forward_physical()` ensures X in [0,1] and M > 0; `forward()` returns log-space for data loss.\n3. **Residual blocks + LayerNorm** (2.3.3): Stable gradient flow in the delta network.\n4. **Fourier features** (2.3.4): Optional positional encoding to reduce spectral bias (flag-gated, off by default).\n5. **Xavier initialization** (2.3.5): Matched to tanh activations.\n6. **Physical-space Jacobian matching** (6.7 Option A): Jacobians compared in physical units for both theory and PCINNv2.\n\nSame hyperparameters as the baseline PCINN (Adam, lr=3e-4, 10k epochs, 32 Jacobian samples) for fair comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FourierFeatures(nn.Module):\n    \"\"\"Random Fourier feature mapping for low-dimensional inputs.\n\n    Replaces raw inputs with sin/cos of random projections to reduce\n    spectral bias (Tancik et al., NeurIPS 2020).\n\n    Input:  (batch, in_dim)\n    Output: (batch, 2 * num_frequencies)\n    \"\"\"\n\n    def __init__(self, in_dim: int, num_frequencies: int = 16, scale: float = 2.0) -> None:\n        super().__init__()\n        self.register_buffer(\"B\", torch.randn(in_dim, num_frequencies) * scale)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        proj = x @ self.B\n        return torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Pre-norm residual block: LayerNorm → Linear → Tanh → Linear + skip.\"\"\"\n\n    def __init__(self, dim: int) -> None:\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.fc1 = nn.Linear(dim, dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.fc2 = nn.Linear(dim, dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        h = torch.tanh(self.fc1(self.norm1(x)))\n        h = self.fc2(self.norm2(h))\n        return torch.tanh(x + h)\n\n\nclass PCINNv2(nn.Module):\n    \"\"\"Residual-correction PCINN with physical output constraints.\n\n    y(x) = theory(x) + delta(concat(x_repr, theory(x)))\n\n    where x_repr is either raw x or Fourier-encoded x, and delta is a\n    small network with a residual block and LayerNorm.\n\n    Inputs:  (batch, 5) — [M], [S], [I], temp, time (scaled 0–1)\n\n    forward_physical(x) → (batch, 6): X ∈ [0,1], M1..M5 > 0 (physical units)\n    forward(x)          → (batch, 6): X, log10(M1)..log10(M5) (loss space)\n    \"\"\"\n\n    def __init__(\n        self,\n        theory_model: nn.Module,\n        hidden: int = 64,\n        use_fourier: bool = False,\n        num_frequencies: int = 16,\n    ) -> None:\n        super().__init__()\n        self.theory = theory_model\n\n        self.use_fourier = use_fourier\n        if use_fourier:\n            self.fourier = FourierFeatures(5, num_frequencies)\n            x_repr_dim = 2 * num_frequencies\n        else:\n            x_repr_dim = 5\n\n        # Delta network input: x_repr + theory(x) outputs\n        delta_in_dim = x_repr_dim + 6\n\n        self.input_proj = nn.Sequential(\n            nn.Linear(delta_in_dim, hidden),\n            nn.LayerNorm(hidden),\n            nn.Tanh(),\n        )\n        self.res_block = ResidualBlock(hidden)\n\n        # Split output heads for physical consistency\n        self.X_head = nn.Linear(hidden, 1)   # additive correction to X\n        self.M_head = nn.Linear(hidden, 5)   # log-multiplicative correction to M\n\n        self._init_delta_weights()\n\n    def _init_delta_weights(self) -> None:\n        \"\"\"Xavier init matched to tanh activations (Section 2.3.5).\"\"\"\n        gain = nn.init.calculate_gain(\"tanh\")\n        for m in self.modules():\n            if m is self.theory:\n                continue  # skip frozen theory model\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=gain)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward_physical(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Physical-space output: X ∈ [0,1], M > 0.\"\"\"\n        theory_out = self.theory(x)  # (batch, 6) or (6,) under vmap\n\n        if self.use_fourier:\n            x_repr = self.fourier(x)\n        else:\n            x_repr = x\n\n        delta_in = torch.cat([x_repr, theory_out], dim=-1)\n        h = self.input_proj(delta_in)\n        h = self.res_block(h)\n\n        # X: additive correction, clamped to [0, 1]\n        # Use [..., :1] so this works both batched (2D) and unbatched (1D, from vmap/jacrev)\n        X = torch.clamp(theory_out[..., :1] + self.X_head(h), 0.0, 1.0)\n\n        # M: multiplicative correction via exp(delta) ensures positivity\n        # When delta ≈ 0 (at init), output ≈ theory output\n        M = theory_out[..., 1:] * torch.exp(self.M_head(h))\n\n        return torch.cat([X, M], dim=-1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Loss-space output: X, log10(M1)..log10(M5).\"\"\"\n        phys = self.forward_physical(x)\n        # Use [..., :1] so this works both batched (2D) and unbatched (1D, from vmap/jacrev)\n        X = phys[..., :1]\n        M_log = torch.log10(phys[..., 1:] + 1e-12)\n        return torch.cat([X, M_log], dim=-1)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "USE_FOURIER = False  # Set True to enable Fourier feature encoding\n\npcinnv2_model = PCINNv2(\n    theory_model=Domain_NN,\n    hidden=64,\n    use_fourier=USE_FOURIER,\n).to(DEVICE)\n\nloss_function = nn.MSELoss()\n# Only optimize delta network parameters (theory is frozen inside PCINNv2)\ntrainable_params = [p for p in pcinnv2_model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(trainable_params, lr=3e-4)\n\nEPOCHS = 10_000\nTOTAL_JAC_SAMPLES = 32\n\nv2_train_losses: list[float] = []\nv2_test_losses: list[float] = []\nv2_jac_losses: list[float] = []\n\nprint(f\"PCINNv2 trainable params: {sum(p.numel() for p in trainable_params):,}\")\nprint(f\"Fourier features: {USE_FOURIER}\")\n\nfor epoch in range(EPOCHS):\n    pcinnv2_model.train()\n    optimizer.zero_grad(set_to_none=True)\n\n    # Data loss in log-space (same as baseline PCINN)\n    pred = pcinnv2_model(Xtrainsample)\n    trainloss = loss_function(pred, Ytrainsample)\n\n    # --- Jacobian sampling ---\n    Msample = M_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    Ssample = 10 - Msample\n    Isample = I_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    Tsample = T_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n    tsample = time_sampler.sample((TOTAL_JAC_SAMPLES, 1))\n\n    sampl = torch.cat((Msample, Ssample, Isample, Tsample, tsample), dim=1).to(DEVICE)\n    sampl = (sampl - SCALERX_MIN) / SCALERX_DENOM\n    sampl = sampl.float().detach().requires_grad_(True)\n\n    # Jacobian matching in PHYSICAL space (Section 6.7 Option A)\n    # Theory Jacobian (already in physical units)\n    jac_theory_sampl = vmap(jacrev(Domain_NN))(sampl).detach()\n    # PCINNv2 Jacobian via forward_physical (also physical units)\n    jac_sampl = vmap(jacrev(pcinnv2_model.forward_physical))(sampl)\n\n    jacloss = loss_function(jac_sampl, jac_theory_sampl)\n\n    loss = trainloss + jacloss\n    loss.backward()\n    optimizer.step()\n\n    v2_train_losses.append(trainloss.item())\n    v2_jac_losses.append(jacloss.item())\n\n    pcinnv2_model.eval()\n    with torch.inference_mode():\n        test_pred = pcinnv2_model(Xtestsample)\n        testloss = loss_function(test_pred, Ytestsample).item()\n    v2_test_losses.append(testloss)\n\n    if (epoch + 1) % 500 == 0:\n        print(\n            f\"[PCINNv2] epoch={epoch+1}/{EPOCHS} \"\n            f\"obj={v2_train_losses[-1]:.6g} jac={v2_jac_losses[-1]:.6g} test={v2_test_losses[-1]:.6g}\"\n        )\n\nV2pred = pcinnv2_model(Xtestsample)\n\n# --- Comparison plot: all three models ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: train + test loss\naxes[0].plot(safe_log(train_losses), label=\"Baseline train\", alpha=0.7)\naxes[0].plot(safe_log(test_losses), label=\"Baseline test\", alpha=0.7)\naxes[0].plot(safe_log(pcinn_train_losses), label=\"PCINN train\", alpha=0.7)\naxes[0].plot(safe_log(pcinn_test_losses), label=\"PCINN test\", alpha=0.7)\naxes[0].plot(safe_log(v2_train_losses), label=\"PCINNv2 train\", alpha=0.7)\naxes[0].plot(safe_log(v2_test_losses), label=\"PCINNv2 test\", alpha=0.7)\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"log(loss)\")\naxes[0].set_title(\"Train/Test Loss Comparison\")\naxes[0].grid(True, alpha=0.3)\naxes[0].legend(fontsize=8)\n\n# Right: Jacobian losses\naxes[1].plot(safe_log(jac_losses), label=\"PCINN jac\", alpha=0.7)\naxes[1].plot(safe_log(v2_jac_losses), label=\"PCINNv2 jac\", alpha=0.7)\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"log(jac loss)\")\naxes[1].set_title(\"Jacobian Loss Comparison\")\naxes[1].grid(True, alpha=0.3)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal test losses:\")\nprint(f\"  Baseline NN: {test_losses[-1]:.6g}\")\nprint(f\"  PCINN:       {pcinn_test_losses[-1]:.6g}\")\nprint(f\"  PCINNv2:     {v2_test_losses[-1]:.6g}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Compare the train/test loss curves from both models above. The PCINN should show lower test loss than the baseline NN, demonstrating that Jacobian regularization from the theory model improves generalization. To reproduce the full leave-one-experiment-out cross-validation results from paper Table 3, re-run this notebook with `TestReaction` set to each integer from 1 to 8 and aggregate the per-reaction test errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}