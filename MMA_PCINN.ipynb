{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymer Chemistry Informed Neural Networks (PCINNs)\n",
    "\n",
    "**Paper:** Ballard, N. *Polymer Chemistry Informed Neural Networks.* *Polym. Chem.*, 2024. DOI: [10.1039/D4PY00995A](https://doi.org/10.1039/D4PY00995A)\n",
    "\n",
    "**GitHub:** [PolymatGIQ/PCINN](https://github.com/PolymatGIQ/PCINN)\n",
    "\n",
    "This notebook implements the PCINN method for predicting methyl methacrylate (MMA) free-radical polymerization outcomes. The approach combines a data-driven neural network with Jacobian-based regularization from a pretrained kinetic (\"theory\") model. The theory model provides gradient information that guides the trainable network toward physically consistent predictions, yielding improved generalization compared to a purely data-driven baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pinned dependencies (verified compatible as of February 2026).\n",
    "# On systems with CUDA, follow https://pytorch.org/get-started/locally/\n",
    "# to install the CUDA-enabled PyTorch build instead of the default CPU-only wheel.\n",
    "# If a pin fails to resolve, check PyPI for the latest patch in the same minor\n",
    "# series (e.g., numpy==2.4.3 if 2.4.2 is superseded).\n",
    "%pip install torch==2.10.0 numpy==2.4.2 pandas==2.2.3 matplotlib==3.10.8 openpyxl==3.1.5 \"ipykernel>=6.29,<7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.func import vmap, jacrev\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "Two neural network architectures are used. **`NNmodel`** is the trainable predictive network (5 inputs → 128 → 64 → 6 outputs, tanh activations) used for both the baseline and PCINN experiments. **`DomainModel`** is the pretrained theory surrogate (5 → 128 → 128 → 64 with split output heads: sigmoid for conversion, softplus for molecular weights) that provides Jacobian targets during PCINN training. See paper Scheme 2 (theory model) and Scheme 3 (PCINN architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNmodel(nn.Module):  # Main network\n",
    "    def __init__(self):\n",
    "        super(NNmodel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DomainModel(nn.Module):  # \"Theory network\"\n",
    "    def __init__(self):\n",
    "        super(DomainModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.Xout = nn.Linear(64, 1)\n",
    "        self.Mout = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        X_out = torch.sigmoid(self.Xout(x))  # conversion\n",
    "        M_out = F.softplus(self.Mout(x))  # Mn, Mw, Mz, Mz+1, Mv\n",
    "\n",
    "        return torch.cat((X_out, M_out), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Input features are scaled to the [0, 1] range using precomputed per-feature min and max values stored in `scalerx_min.npy` and `scalerx_max.npy`. The five molecular weight outputs (Mn, Mw, Mz, Mz+1, Mv) are log10-transformed before training to reduce scale differences, while conversion (X) is left on its original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalefeaturezeroone(x, scalerxmax, scalerxmin):\n",
    "    max_minus_min = (scalerxmax - scalerxmin)\n",
    "    return (x - scalerxmin) / max_minus_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerx_max = np.load('scalerx_max.npy')\n",
    "scalerx_min = np.load('scalerx_min.npy')\n",
    "\n",
    "# load training data\n",
    "df = pd.read_excel('PMMAordered.xlsx')\n",
    "\n",
    "dfX = df[[\"[M]\", \"[S]\", \"[I]\", \"temp\", \"time\", \"Reaction\"]]\n",
    "dfY = df[[\"X\", \"Mn\", \"Mw\", \"Mz\", \"Mzplus1\", \"Mv\"]]\n",
    "\n",
    "Xdata = dfX.values\n",
    "Ydata = dfY.values\n",
    "\n",
    "Ydata[:, 1:] = np.log10(Ydata[:, 1:])\n",
    "Xdata[:, :5] = scalefeaturezeroone(Xdata[:, :5], scalerx_max, scalerx_min)\n",
    "\n",
    "dfGPC = df.iloc[:, 18:]\n",
    "GPCdata = dfGPC.values\n",
    "\n",
    "Domain_NN = DomainModel()\n",
    "import pickle\n",
    "import _pickle\n",
    "\n",
    "try:\n",
    "    # Secure path first (PyTorch 2.6+ default).\n",
    "    Domain_NN.load_state_dict(torch.load('MMA_solution_net.pt', weights_only=True))\n",
    "except (pickle.UnpicklingError, _pickle.UnpicklingError):\n",
    "    # Fallback for older pickle-based checkpoints; safe here because\n",
    "    # this repository controls the checkpoint provenance.\n",
    "    Domain_NN.load_state_dict(torch.load('MMA_solution_net.pt', weights_only=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Jacobian Sampling Configuration\n\nDuring PCINN training, 32 random points are sampled per epoch from the physical domain to compute Jacobian matching loss. The sampling ranges correspond to experimentally relevant conditions: temperature 323–363 K (50–90 °C), monomer concentration [M] 0.5–5 mol/L, initiator concentration [I] 0.005–0.1 mol/L, and reaction time 300–36,000 s (5 min to 10 hours). See paper eqn (9) for the combined loss function.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "Tupper = 273 + 90\nTlower = 273 + 50\n\nMupper = 5\nMlower = 0.5\n\nIupper = 0.1\nIlower = 0.005\n\ntimeupper = 10 * 60 * 60\ntimelower = 5 * 60\n\nM_sampler = torch.distributions.Uniform(low=Mlower, high=Mupper)\nT_sampler = torch.distributions.Uniform(low=Tlower, high=Tupper)\nI_sampler = torch.distributions.Uniform(low=Ilower, high=Iupper)\ntime_sampler = torch.distributions.Uniform(low=timelower, high=timeupper)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training Configuration\n\nA leave-one-experiment-out cross-validation strategy is used: the dataset contains 8 distinct polymerization reactions, and `TestReaction` selects which reaction is held out for testing while the remaining 7 are used for training. To reproduce the full cross-validation results from paper Table 3, run this notebook 8 times with `TestReaction` set to each value from 1 to 8.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "TestReaction = 8  # Integer between 1 and 8. Picks reaction that will be tested on (other reactions for training)\n\n# get training samples\nXtrainsample = Xdata[Xdata[:, 5] != TestReaction]\nXtrainsample = Xtrainsample[:, :5]\nXtrainsample = torch.from_numpy(Xtrainsample).float()\nYtrainsample = Ydata[Xdata[:, 5] != TestReaction]\nYtrainsample = torch.from_numpy(Ytrainsample).float()\n\n# get test samples\nXtestsample = Xdata[Xdata[:, 5] == TestReaction]\nXtestsample = Xtestsample[:, :5]\nXtestsample = torch.from_numpy(Xtestsample).float()\nYtestsample = Ydata[Xdata[:, 5] == TestReaction]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Baseline Neural Network Training\n\nThis cell trains a conventional data-only neural network (no Jacobian regularization) as a control comparison. The model is trained for 10,000 epochs with Adam (lr=3e-4) and MSE loss on the training data only. This serves as the baseline against which the PCINN is compared in paper Table 3.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = NNmodel()\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3E-4)\n\nepochs = 10000\nreg_losses = []\nreg_losses_test = []\nfor epoch in range(epochs):\n    Sum_Obj_loss = 0\n    pred = model(Xtrainsample)\n    trainloss = loss_function(pred, Ytrainsample)\n    Sum_Obj_loss += trainloss\n    if epoch >= 1:\n        # Backpropagation\n        trainloss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    pred = model(Xtestsample)\n    testloss = loss_function(pred, torch.from_numpy(Ytestsample).float())\n    reg_losses.append(float(Sum_Obj_loss))\n    reg_losses_test.append(float(testloss))\n\nNNpred = model(Xtestsample)\n\nplt.plot(np.log(reg_losses), label='Training loss')\nplt.plot(np.log(reg_losses_test), label='Test loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## PCINN Training\n\nThe PCINN augments the standard data-driven MSE loss with a Jacobian matching term (see paper eqn 9). At each epoch, 32 random points are sampled from the physical domain and the Jacobian of both the trainable network and the pretrained theory network are computed using `vmap(jacrev(...))`. The MSE between these Jacobians is added to the data loss, encouraging the PCINN to learn input-output sensitivities consistent with the underlying kinetic model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "PCINNmodel = NNmodel()\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(PCINNmodel.parameters(), lr=3E-4)\n\nepochs = 10000\n\nreg_losses = []\nreg_losses_test = []\n\ntotaljacsamples = 32\n\nfor epoch in range(epochs):\n    Sum_Jac_loss = 0\n    Sum_Obj_loss = 0\n\n    pred = PCINNmodel(Xtrainsample)\n    trainloss = loss_function(pred, Ytrainsample)\n    Sum_Obj_loss += trainloss\n\n    Msample = M_sampler.sample((32, 1))\n    Ssample = 10 - Msample\n    Isample = I_sampler.sample((32, 1))\n    Tsample = T_sampler.sample((32, 1))\n    tsample = time_sampler.sample((32, 1))\n    sampl = torch.cat((Msample, Ssample, Isample, Tsample, tsample), 1)\n    sampl = (sampl - scalerx_min) / (scalerx_max - scalerx_min)\n\n    jac_theory_sampl = vmap(jacrev(Domain_NN))(sampl.float())\n    jac_sampl = vmap(jacrev(PCINNmodel))(sampl.float())\n\n    jacloss = loss_function(jac_sampl, jac_theory_sampl)\n    Sum_Jac_loss += jacloss\n\n    loss = trainloss + jacloss\n\n    if epoch >= 1:\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    reg_losses.append(float(Sum_Obj_loss))\n    pred = PCINNmodel(Xtestsample)\n    testloss = loss_function(pred, torch.from_numpy(Ytestsample).float())\n    reg_losses_test.append(float(testloss))\n\nEBNNpred = PCINNmodel(Xtestsample)\n\nplt.plot(np.log(reg_losses), label='Training loss')\nplt.plot(np.log(reg_losses_test), label='Test loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Results Summary\n\nCompare the train/test loss curves from both models above. The PCINN should show lower test loss than the baseline NN, demonstrating that Jacobian regularization from the theory model improves generalization. To reproduce the full leave-one-experiment-out cross-validation results from paper Table 3, re-run this notebook with `TestReaction` set to each integer from 1 to 8 and aggregate the per-reaction test errors.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
